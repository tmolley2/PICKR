{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f895b84a-418b-43ee-ae0e-20c8295acf5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HGNC symbols...\n",
      "Loaded 193359 HGNC symbols.\n",
      "\n",
      "============================================================\n",
      "STARTING PROCESSING FOR YEAR: 2025\n",
      "============================================================\n",
      "\n",
      "Searching Europe PMC with query: OPEN_ACCESS:y AND HAS_FT:y AND (METHODS:\"qPCR\" OR \"RT-PCR\" OR \"real time PCR\") AND (ABSTRACT:\"pluripotent\" OR \"iPSC\" OR \"PSC\" OR \"hPSC\" OR \"ESC\" OR \"hESC\") AND (FIRST_PDATE:2025)\n",
      "Retrieved 500 records for 2025.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Papers for 2025:   0%|                                                      | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Processing PMCID: PMC12076839 (1/500) for year 2025\n",
      "  Found 0 <table> elements in XML for PMC12076839.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Supps for PMC12076839:   0%|                                                             | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Supps for PMC12076839:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 1/2 [00:00<00:00,  1.48it/s]\u001b[A\n",
      "                                                                                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sufficient primers (42) now found for PMC12076839. Skipping its remaining supplements.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Papers for 2025:   0%|                                            | 1/500 [00:09<1:20:37,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Processing PMCID: PMC12008852 (2/500) for year 2025\n",
      "  Found 0 <table> elements in XML for PMC12008852.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Papers for 2025:   0%|                                            | 1/500 [00:45<6:15:52, 45.19s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1179\u001b[39m\n\u001b[32m   1176\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m      Sufficient primers (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprimers_found_for_this_pmcid_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_pmcid_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from XML. Skipping its supplement processing.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1177\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1178\u001b[39m     \u001b[38;5;66;03m# print(f\"    Fetching supplements for {current_pmcid_str} (XML found: {primers_found_for_this_pmcid_count})...\")\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1179\u001b[39m     downloaded_supp_files = \u001b[43mfetch_supplements_from_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_pmcid_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxml_file_for_current_paper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msupp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m downloaded_supp_files:\n\u001b[32m   1182\u001b[39m         renamed_supp_files_for_this_pmcid = rename_supp_files(current_pmcid_str, downloaded_supp_files)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 254\u001b[39m, in \u001b[36mfetch_supplements_from_xml\u001b[39m\u001b[34m(pmcid, xml_path, dest_dir, pause)\u001b[39m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m     r = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHEADERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m     r.raise_for_status()\n\u001b[32m    256\u001b[39m     fpath.write_bytes(r.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\camtest\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\camtest\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\camtest\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\camtest\\Lib\\site-packages\\requests\\sessions.py:724\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[32m    722\u001b[39m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[32m    723\u001b[39m     gen = \u001b[38;5;28mself\u001b[39m.resolve_redirects(r, request, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m     history = \u001b[43m[\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    726\u001b[39m     history = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\camtest\\Lib\\site-packages\\requests\\sessions.py:265\u001b[39m, in \u001b[36mSessionRedirectMixin.resolve_redirects\u001b[39m\u001b[34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m.cookies, prepared_request, resp.raw)\n\u001b[32m    278\u001b[39m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\camtest\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\camtest\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\camtest\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\camtest\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\camtest\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    513\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    519\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\camtest\\Lib\\http\\client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\camtest\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\camtest\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\camtest\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\camtest\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\camtest\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------\n",
    "# 0.  Notebook boiler-plate  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# -----------------------------------------------------------------------------------\n",
    "!pip install --quiet requests lxml tqdm pandas\n",
    "\n",
    "import json, re, time, requests, os, gzip\n",
    "import camelot\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pdfplumber # Import pdfplumber\n",
    "import pandas as pd, textwrap, os, sys, pprint, itertools\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "from lxml import etree        # robust XML parser\n",
    "from tqdm import tqdm # progress-bar in Jupyter\n",
    "from pebble import ProcessPool\n",
    "from concurrent.futures import TimeoutError as PebbleTimeoutError\n",
    "import docx \n",
    "\n",
    "\n",
    "# --- Configuration: Keywords and Regular Expressions ---\n",
    "FORWARD_KEYWORDS = ['forward', 'fw', 'fp', 'fwd', 'sense', 'forw']\n",
    "REVERSE_KEYWORDS = ['reverse', 'rev', 'rp', 'rv', 'antisense', 'revs']\n",
    "GENE_HEADER_KEYWORDS = ['gene', 'target', 'name', 'symbol', 'oligonucleotide', 'oligos', 'locus']\n",
    "ALLOWED_DNA_CHARS_FOR_FILTER = \"ACTG\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 0.5  abhi, change htis directoy hereâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# -----------------------------------------------------------------------------------\n",
    "DATA_DIR = Path(\"data/psc\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR = Path(\"data/psc/output\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Europe PMC imposes a 30 req/min soft limit â†’ be polite.\n",
    "HEADERS   = {\"User-Agent\": \"PrimerMiner/0.1 (tmolley@ucsd.edu)\"}\n",
    "BASE_URL  = \"https://www.ebi.ac.uk/europepmc/webservices/rest/\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 1.  Search Europe PMC for the latest 10 OA papers mentioning â€œGAPDHâ€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# -----------------------------------------------------------------------------------\n",
    "def cleanup_downloaded_files(dirs_to_clean: List[Path]):\n",
    "    \"\"\"\n",
    "    Deletes all files within a list of specified directories.\n",
    "    This is used to manage storage space between batch runs.\n",
    "    \"\"\"\n",
    "    for directory in dirs_to_clean:\n",
    "        if not directory.is_dir():\n",
    "            print(f\"Cleanup skipped: Directory '{directory}' does not exist.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  Cleaning files in: {directory}\")\n",
    "        files_deleted = 0\n",
    "        for item in directory.iterdir():\n",
    "            # Ensure we only delete files, not subdirectories\n",
    "            if item.is_file():\n",
    "                try:\n",
    "                    item.unlink()\n",
    "                    files_deleted += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"    Could not delete file {item}: {e}\")\n",
    "        print(f\"    ...deleted {files_deleted} file(s).\")\n",
    "        \n",
    "def europepmc_search_all(query: str,\n",
    "                         max_records: int = 10000000,\n",
    "                         page_size: int = 1000,\n",
    "                         throttle: float = 0.3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch up to `max_records` hits for `query`, paging with Europe PMC's\n",
    "    cursorMark API. Returns a list of JSON result dicts.\n",
    "    This version is updated to handle cases where no 'nextCursorMark' is returned.\n",
    "    \"\"\"\n",
    "    hits, retrieved = [], 0\n",
    "    cursor = \"*\"\n",
    "\n",
    "    while retrieved < max_records:\n",
    "        url = (f\"{BASE_URL}search\"\n",
    "               f\"?query={requests.utils.quote(query)}\"\n",
    "               f\"&format=json\"\n",
    "               f\"&pageSize={page_size}\"\n",
    "               f\"&cursorMark={cursor}\")\n",
    "        r = requests.get(url, headers=HEADERS, timeout=40)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "\n",
    "        if \"resultList\" not in data:\n",
    "            raise RuntimeError(f\"Europe PMC API error: {data.get('error', data)}\")\n",
    "\n",
    "        batch = data[\"resultList\"][\"result\"]\n",
    "        if not batch:  # no more results\n",
    "            break\n",
    "\n",
    "        hits.extend(batch)\n",
    "        retrieved += len(batch)\n",
    "        \n",
    "        # --- MODIFICATION ---\n",
    "        # Gracefully handle the end of results when 'nextCursorMark' is absent.\n",
    "        # The .get() method returns None if the key is not found, preventing a KeyError.\n",
    "        cursor = data.get(\"nextCursorMark\")\n",
    "        if not cursor:\n",
    "            # No more pages to fetch, so we exit the loop.\n",
    "            break\n",
    "        # --- END MODIFICATION ---\n",
    "\n",
    "        time.sleep(throttle)  # be polite (â‰¤ 30 req / min)\n",
    "\n",
    "    return hits[:max_records]\n",
    "    \n",
    "def load_hgnc_symbol_set(cache_dir=\"hgnc_cache\") -> set[str]:\n",
    "    # Use Path directly instead of pathlib.Path\n",
    "    cache_path = Path(cache_dir) / \"homo_sapiens.gene_info.gz\"\n",
    "    if not cache_path.exists():\n",
    "        cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        url = (\"https://ftp.ncbi.nlm.nih.gov/gene/DATA/GENE_INFO/\"\n",
    "               \"Mammalia/Homo_sapiens.gene_info.gz\")\n",
    "        print(\"â¬  downloading HGNC symbol list â€¦\") # Corrected emoji display if needed\n",
    "        try:\n",
    "            r = requests.get(url, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            cache_path.write_bytes(r.content)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading HGNC symbols: {e}\")\n",
    "            print(\"Proceeding without HGNC symbol validation for XML gene names.\")\n",
    "            return set() # Return an empty set if download fails\n",
    "\n",
    "    symbols = set()\n",
    "    try:\n",
    "        with gzip.open(cache_path, \"rt\", encoding='utf-8') as fh: # Added encoding\n",
    "            for line in fh:\n",
    "                if line.startswith(\"#\"):\n",
    "                    continue\n",
    "                cols = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                if len(cols) > 2: # Ensure there are enough columns\n",
    "                    symbols.add(cols[2].upper())  # official symbol column\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading or processing HGNC file {cache_path}: {e}\")\n",
    "        print(\"Proceeding without HGNC symbol validation for XML gene names.\")\n",
    "        return set() # Return an empty set if file processing fails\n",
    "    \n",
    "    if not symbols:\n",
    "        print(f\"Warning: HGNC symbol set is empty after attempting to load from {cache_path}.\")\n",
    "    else:\n",
    "        print(f\"Loaded {len(symbols)} HGNC symbols.\")\n",
    "    return symbols\n",
    "    \n",
    "# ---------------------------------------------------------------------------\n",
    "# 3  Helper: fetch XML only if we don't have it already\n",
    "# ---------------------------------------------------------------------------\n",
    "def get_or_download_xml(pmcid: str, dest_dir: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Return the path to the XML file for `pmcid`.\n",
    "    If the file is already on disk, just return the path.\n",
    "    Otherwise download from Europe PMC and save it.\n",
    "    \"\"\"\n",
    "    fpath = dest_dir / f\"{pmcid}.xml\"\n",
    "    if fpath.exists():                # âœ… cached\n",
    "        return fpath\n",
    "\n",
    "    # ðŸ ž not cached â†’ download\n",
    "    url = f\"{BASE_URL}{pmcid}/fullTextXML\"\n",
    "    r   = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    fpath.write_text(r.text, encoding=\"utf-8\")\n",
    "    return fpath\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 4.  Download full-text XML and supplement for each PMCID  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# -----------------------------------------------------------------------------------\n",
    "def fetch_fulltext_xml(pmcid: str) -> str:\n",
    "    \"\"\"Return the raw JATS XML for a given PMCID from Europe PMC.\"\"\"\n",
    "    url = f\"{BASE_URL}{pmcid}/fullTextXML\"\n",
    "    r   = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def save_xml(pmcid: str, xml_text: str) -> Path:\n",
    "    \"\"\"Persist the XML to disk and return the file path.\"\"\"\n",
    "    fpath = DATA_DIR / f\"{pmcid}.xml\"\n",
    "    fpath.write_text(xml_text, encoding=\"utf-8\")\n",
    "    return fpath\n",
    "    \n",
    "def safe_json(resp):\n",
    "    \"\"\"Return resp.json() or an empty dict if the payload isnâ€™t JSON.\"\"\"\n",
    "    try:\n",
    "        if resp.text.strip():\n",
    "            return resp.json()\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    return {}\n",
    "    \n",
    "def rename_supp_files(pmcid: str, file_paths: list[Path]) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Rename each Path in `file_paths` to <pmcid>_supp<N>.<ext>\n",
    "    Returns a list of new Path objects (same length/order as input).\n",
    "    \"\"\"\n",
    "    new_paths = []\n",
    "    for i, old_path in enumerate(sorted(file_paths), 1):\n",
    "        new_name = f\"{pmcid}_supp{i}{old_path.suffix.lower()}\"\n",
    "        new_path = old_path.with_name(new_name)\n",
    "        if new_path.exists():            # avoid accidental overwrite\n",
    "            new_path = old_path          # keep original\n",
    "        else:\n",
    "            old_path.rename(new_path)\n",
    "        new_paths.append(new_path)\n",
    "    return new_paths\n",
    "    \n",
    "def fetch_supplements_from_xml(pmcid: str,\n",
    "                               xml_path: Path,\n",
    "                               dest_dir: Path,\n",
    "                               pause: float = 0.2) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Download *only* PDF supplementary files referenced in the JATS XML.\n",
    "    \"\"\"\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if not xml_path.exists():\n",
    "        print(f\"âš ï¸  XML file missing for {pmcid}\")\n",
    "        return []\n",
    "\n",
    "    # ---- 1. gather every xlink:href ----------------------------------------\n",
    "    root = etree.parse(str(xml_path))\n",
    "    ns   = {\"x\": \"http://www.w3.org/1999/xlink\"}\n",
    "    hrefs = set()\n",
    "    hrefs.update(root.xpath(\"//supplementary-material/@x:href\", namespaces=ns))\n",
    "    hrefs.update(root.xpath(\"//supplementary-material//media/@x:href\", namespaces=ns))\n",
    "    hrefs.update(root.xpath(\"//ext-link[contains(@ext-link-type,'supplement')]/@x:href\",\n",
    "                            namespaces=ns))\n",
    "\n",
    "    if not hrefs:\n",
    "        return []\n",
    "\n",
    "    # ---- 2. download PDF files only ---------------------------------------\n",
    "    base  = f\"https://europepmc.org/articles/{pmcid}/bin/\"\n",
    "    saved = []\n",
    "\n",
    "    for href in hrefs:\n",
    "        url   = href if href.startswith(\"http\") else base + href.lstrip(\"/\")\n",
    "        fname = url.split(\"/\")[-1].split(\"?\")[0]\n",
    "\n",
    "        # **PDF filter**\n",
    "        if not fname.lower().endswith((\".pdf\", \".docx\")):\n",
    "            continue\n",
    "\n",
    "        fpath = dest_dir / fname\n",
    "        if fpath.exists():\n",
    "            saved.append(fpath)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            fpath.write_bytes(r.content)\n",
    "            saved.append(fpath)\n",
    "            time.sleep(pause)\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"âš ï¸  {pmcid}  {fname} â†’\", e)\n",
    "\n",
    "    return saved\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 5.  supplmental reader functions  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "def parse_docx_table_to_dataframe(docx_table_obj):\n",
    "    \"\"\"Converts a python-docx table object to a pandas DataFrame.\"\"\"\n",
    "    data = []\n",
    "    keys = None\n",
    "    \n",
    "    # Attempt to use the first row as headers\n",
    "    if docx_table_obj.rows:\n",
    "        potential_header_cells = [cell.text.strip() for cell in docx_table_obj.rows[0].cells]\n",
    "        # Crude check: if not too many empty cells and not overly long strings, assume header\n",
    "        if len(potential_header_cells) > 0 and \\\n",
    "           sum(1 for h in potential_header_cells if not h) < len(potential_header_cells) / 2 and \\\n",
    "           all(len(h) < 100 for h in potential_header_cells): # Avoid very long cell content as header\n",
    "            keys = potential_header_cells\n",
    "            \n",
    "    for i, row in enumerate(docx_table_obj.rows):\n",
    "        text = [cell.text.strip() for cell in row.cells]\n",
    "        if keys and i == 0 and len(docx_table_obj.rows) > 1: # If keys were taken from first row and there are more rows\n",
    "            continue # Skip adding the header row to data list\n",
    "        \n",
    "        # If keys were set and match current row length, create a dict for this row\n",
    "        if keys and len(text) == len(keys) and (i > 0 or len(docx_table_obj.rows) == 1):\n",
    "             data.append(dict(zip(keys, text)))\n",
    "        else: # Fallback to list of lists if no keys or length mismatch\n",
    "            data.append(text) # Will have default integer columns if this path is taken mostly\n",
    "\n",
    "    if not data:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        df = pd.DataFrame(data)\n",
    "        # If dataframe was created from list of lists and keys were determined but not used as columns yet\n",
    "        if keys and not isinstance(data[0], dict) and len(keys) == df.shape[1]:\n",
    "            df.columns = keys\n",
    "    except Exception: # Fallback if DataFrame creation is problematic\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    # If after all that, columns are still default integers, and first data row looks like header\n",
    "    if not df.empty and all(isinstance(col, int) for col in df.columns) and df.shape[0] > 0:\n",
    "        first_data_row_values = [str(c).strip() for c in df.iloc[0]]\n",
    "        # Simple heuristic: if first row doesn't contain typical long sequences and has text\n",
    "        is_first_row_likely_header = not any(is_valid_primer_sequence(extract_sequence_and_direction_from_cell(str(c))[0] or \"\", 18, 40) for c in first_data_row_values) and \\\n",
    "                                   any(len(str(c)) > 0 for c in first_data_row_values)\n",
    "        if is_first_row_likely_header and df.shape[0] > 1:\n",
    "            df.columns = first_data_row_values\n",
    "            df = df[1:].reset_index(drop=True)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def extract_primers_from_docx(docx_path: Path, pmcid: str, hgnc_symbols: set) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extracts primer sequences and context from DOCX files (tables and prose).\n",
    "    \"\"\"\n",
    "    found_docx_primers = []\n",
    "    if not docx_path.exists():\n",
    "        return found_docx_primers\n",
    "\n",
    "    try:\n",
    "        doc = docx.Document(docx_path) # from python-docx library\n",
    "    except Exception as e:\n",
    "        print(f\"    Error opening DOCX file {docx_path.name}: {e}\")\n",
    "        return found_docx_primers\n",
    "\n",
    "    # --- 1. Process Tables in DOCX ---\n",
    "    if doc.tables:\n",
    "        # print(f\"    Found {len(doc.tables)} table(s) in DOCX: {docx_path.name}\")\n",
    "        for table_idx, docx_table_obj in enumerate(doc.tables):\n",
    "            # print(f\"      Processing DOCX Table {table_idx+1}...\")\n",
    "            df_from_docx = parse_docx_table_to_dataframe(docx_table_obj)\n",
    "            if not df_from_docx.empty:\n",
    "                # print(f\"        Parsed DOCX table to DataFrame (shape: {df_from_docx.shape})\")\n",
    "                # Use your existing scan_dataframe_for_primers function\n",
    "                source_desc = f\"DOCX Table {table_idx+1}\" # Page info is not direct like PDF\n",
    "                primers_from_table = scan_dataframe_for_primers(\n",
    "                    df_from_docx, \n",
    "                    pmcid,\n",
    "                    source_desc, \n",
    "                    docx_path.name, # Source File\n",
    "                    hgnc_symbols\n",
    "                )\n",
    "                if primers_from_table:\n",
    "                    found_docx_primers.extend(primers_from_table)\n",
    "            # else:\n",
    "                # print(f\"      DOCX Table {table_idx+1} resulted in an empty DataFrame.\")\n",
    "\n",
    "    # --- 2. Process Prose Text (paragraphs) in DOCX ---\n",
    "    # print(f\"    Scanning {len(doc.paragraphs)} paragraphs in DOCX: {docx_path.name}...\")\n",
    "    unique_prose_blocks_docx = set()\n",
    "    dna_candidate_regex_prose = re.compile(r'\\b((?:[ACGT]\\s*){15,50})\\b', re.IGNORECASE)\n",
    "\n",
    "    for para_idx, para in enumerate(doc.paragraphs):\n",
    "        text_block = para.text.strip()\n",
    "        if not text_block or len(text_block) < 20 or text_block in unique_prose_blocks_docx:\n",
    "            continue\n",
    "        unique_prose_blocks_docx.add(text_block)\n",
    "\n",
    "        for match in dna_candidate_regex_prose.finditer(text_block):\n",
    "            raw_dna_match_with_spaces = match.group(1)\n",
    "            core_bases, direction_from_text_body, _ = \\\n",
    "                extract_sequence_and_direction_from_cell(raw_dna_match_with_spaces)\n",
    "\n",
    "            if core_bases and is_valid_primer_sequence(core_bases):\n",
    "                sequence = core_bases\n",
    "                probable_direction = direction_from_text_body if direction_from_text_body else \"Unknown\"\n",
    "                probable_gene = \"Unknown\"\n",
    "                \n",
    "                match_start_in_block = match.start()\n",
    "                match_end_in_block = match.end()\n",
    "                \n",
    "                context_chars = 80\n",
    "                text_before = text_block[max(0, match_start_in_block - context_chars) : match_start_in_block]\n",
    "                text_after = text_block[match_end_in_block : min(len(text_block), match_end_in_block + context_chars)]\n",
    "                \n",
    "                if probable_direction == \"Unknown\":\n",
    "                    if any(k in text_before.lower() or k in text_after.lower() for k in FORWARD_KEYWORDS):\n",
    "                        probable_direction = \"Forward\"\n",
    "                    elif any(k in text_before.lower() or k in text_after.lower() for k in REVERSE_KEYWORDS):\n",
    "                        probable_direction = \"Reverse\"\n",
    "                \n",
    "                gene_search_context = text_before + raw_dna_match_with_spaces + text_after\n",
    "                potential_gene_matches = re.findall(r'\\b([A-Z][A-Za-z0-9-]{2,15})\\b', gene_search_context) # Case sensitive for initial find\n",
    "                best_dist = float('inf')\n",
    "                common_prose_noise = {\"PCR\", \"DNA\", \"RNA\", \"PRIMER\", \"FORWARD\", \"REVERSE\", \"SEQUENCE\", \"METHOD\", \"FIGURE\", \"TABLE\"} | set(k.upper() for k in FORWARD_KEYWORDS) | set(k.upper() for k in REVERSE_KEYWORDS)\n",
    "\n",
    "                for pg_cand_raw in potential_gene_matches:\n",
    "                    pg_cand_upper = pg_cand_raw.upper()\n",
    "                    # Check if it's an HGNC symbol OR just a plausible non-noise capitalized word\n",
    "                    is_hgnc_or_plausible = (pg_cand_upper in hgnc_symbols) or \\\n",
    "                                           (len(pg_cand_raw) > 2 and pg_cand_upper not in common_prose_noise and not pg_cand_raw.islower())\n",
    "\n",
    "                    if is_hgnc_or_plausible:\n",
    "                        try:\n",
    "                            for m_pg in re.finditer(re.escape(pg_cand_raw), gene_search_context): # Match original case\n",
    "                                primer_rel_start = len(text_before)\n",
    "                                dist = abs(m_pg.start() - primer_rel_start)\n",
    "                                if dist < best_dist and dist < context_chars : \n",
    "                                    best_dist = dist\n",
    "                                    cleaned_pg_cand = clean_gene_name(pg_cand_raw)\n",
    "                                    if cleaned_pg_cand != \"Unknown\": # Ensure clean_gene_name doesn't reject it\n",
    "                                        probable_gene = cleaned_pg_cand\n",
    "                        except: pass \n",
    "                \n",
    "                original_context_snip = f\"...{text_block[max(0,match_start_in_block-30):min(len(text_block),match_end_in_block+30)]}...\"\n",
    "                found_docx_primers.append({\n",
    "                    \"PMCID\": pmcid, \"Gene\": probable_gene, \"Sequence\": sequence,\n",
    "                    \"Orientation\": probable_direction, \"Source File\": docx_path.name, \n",
    "                    \"Page\": f\"DOCX Paragraph\", # Can't get page number easily from python-docx paragraphs\n",
    "                    \"Original Cell Text\": original_context_snip \n",
    "                })\n",
    "    return found_docx_primers\n",
    "\n",
    "    \n",
    "def extract_pmcid_from_filename(filename):\n",
    "    if not isinstance(filename, str):\n",
    "        return \"UnknownPMCID\"\n",
    "    # Regex to capture characters before \"_supp\" followed by anything and \".pdf\"\n",
    "    # This handles variations like _supp1.pdf, _supplemental.pdf, etc.\n",
    "    match = re.match(r\"^(.*?)_supp(?:lemental|lement|s)?\\d*\\.pdf$\", filename, re.IGNORECASE)\n",
    "    if match:\n",
    "        pmcid_candidate = match.group(1)\n",
    "        # Further ensure it looks like a PMCID if that's the strict requirement\n",
    "        if pmcid_candidate.upper().startswith(\"PMC\"):\n",
    "            return pmcid_candidate\n",
    "        # If not starting with PMC, it might be another ID format used before \"_supp\"\n",
    "        return pmcid_candidate \n",
    "    \n",
    "    # Fallback if no \"_supp\" pattern but filename itself is a PMCID\n",
    "    pmc_direct_match = re.match(r\"^(PMC\\d+)\\.pdf$\", filename, re.IGNORECASE)\n",
    "    if pmc_direct_match:\n",
    "        return pmc_direct_match.group(1)\n",
    "\n",
    "    # Generic fallback: return filename without extension if no specific pattern found\n",
    "    return os.path.splitext(filename)[0]\n",
    "\n",
    "\n",
    "def is_plausible_dna_segment(text_segment):\n",
    "    if not isinstance(text_segment, str) or not text_segment.strip():\n",
    "        return False\n",
    "    allowed_interspersed_chars = \" -\" \n",
    "    has_dna_char = False\n",
    "    for char_val in text_segment.upper():\n",
    "        if char_val in ALLOWED_DNA_CHARS_FOR_FILTER:\n",
    "            has_dna_char = True\n",
    "        elif char_val in allowed_interspersed_chars:\n",
    "            continue\n",
    "        elif char_val.isalpha(): \n",
    "            return False \n",
    "    return has_dna_char\n",
    "\n",
    "def extract_sequence_and_direction_from_cell(cell_text):\n",
    "    if not isinstance(cell_text, str) or not cell_text.strip():\n",
    "        return None, None, \"\"\n",
    "\n",
    "    text_body = cell_text.strip()\n",
    "    original_text_for_context = text_body\n",
    "    inferred_direction = None\n",
    "    \n",
    "    # This section for identifying direction keywords (e.g., \"Forward\", \"Rv\") is preserved as it works well.\n",
    "    direction_affixes = {\n",
    "        \"Forward\": [\n",
    "            (r\"^(?:FP[\\s:-]*|FWD[\\s:-]*|FORWARD[\\s:-]*|SENSE[\\s:-]*)\", \"prefix\"),\n",
    "            (r\"\\s*\\((?:Forward|FWD|FP|Sense)\\)$\", \"suffix\")\n",
    "        ],\n",
    "        \"Reverse\": [\n",
    "            (r\"^(?:RP[\\s:-]*|REV[\\s:-]*|REVERSE[\\s:-]*|ANTISENSE[\\s:-]*)\", \"prefix\"),\n",
    "            (r\"\\s*\\((?:Reverse|REV|RP|Antisense)\\)$\", \"suffix\")\n",
    "        ]\n",
    "    }\n",
    "    prime_5_prefix_pattern = r\"^(?:5['`Â´â€˜â€™]?[-â€“â€”]?)\"\n",
    "    prime_3_suffix_pattern = r\"(?:[-â€“â€”]?3['`Â´â€˜â€™]?)$\"\n",
    "\n",
    "    text_body_changed_in_pass = True\n",
    "    while text_body_changed_in_pass:\n",
    "        text_body_before_affix_pass = text_body\n",
    "        text_body_changed_in_pass = False\n",
    "        current_pass_direction_found = False\n",
    "        for dir_key, patterns in direction_affixes.items():\n",
    "            if current_pass_direction_found and inferred_direction: break\n",
    "            for pattern_str, affix_type in patterns:\n",
    "                original_len_text_body = len(text_body)\n",
    "                if affix_type == \"prefix\":\n",
    "                    match = re.match(pattern_str, text_body, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        if not inferred_direction: inferred_direction = dir_key\n",
    "                        text_body = text_body[match.end():].strip()\n",
    "                        current_pass_direction_found = True\n",
    "                elif affix_type == \"suffix\":\n",
    "                    match = re.search(pattern_str, text_body, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        if not (match.start() == 0 and inferred_direction):\n",
    "                            if not inferred_direction: inferred_direction = dir_key\n",
    "                            text_body = text_body[:match.start()].strip()\n",
    "                \n",
    "                if len(text_body) != original_len_text_body:\n",
    "                    text_body_changed_in_pass = True\n",
    "                    if current_pass_direction_found: break\n",
    "            if current_pass_direction_found and inferred_direction: break\n",
    "\n",
    "    text_body = re.sub(prime_5_prefix_pattern, \"\", text_body, flags=re.IGNORECASE).strip()\n",
    "    text_body = re.sub(prime_3_suffix_pattern, \"\", text_body, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    # --- REPLACEMENT LOGIC ---\n",
    "    # The old logic (using is_plausible_dna_segment and filter) is replaced.\n",
    "    # This new regex approach is more robust and solves both of your issues.\n",
    "\n",
    "    # 1. Find all contiguous blocks of valid DNA characters (A, C, T, G).\n",
    "    #    - We set a minimum length (e.g., 10) to avoid matching short acronyms.\n",
    "    #    - This cleanly separates primers from gene names (\"GAPDH\") and ignores junk (\"Ã¢â‚¬Â²\").\n",
    "    primer_candidates = re.findall(r'([ACTG]{10,})', text_body, re.IGNORECASE)\n",
    "    \n",
    "    if not primer_candidates:\n",
    "        return None, inferred_direction, original_text_for_context\n",
    "\n",
    "    # 2. From the candidates found, choose the longest one that passes full validation.\n",
    "    best_sequence = \"\"\n",
    "    for candidate in primer_candidates:\n",
    "        # Your 'is_valid_primer_sequence' function already checks for the correct length (e.g., 18-40).\n",
    "        if is_valid_primer_sequence(candidate) and len(candidate) > len(best_sequence):\n",
    "            best_sequence = candidate\n",
    "    \n",
    "    if best_sequence:\n",
    "        # 3. Return the cleaned, validated, uppercase sequence if found.\n",
    "        return best_sequence.upper(), inferred_direction, original_text_for_context\n",
    "    else:\n",
    "        # No candidate passed the final validation (e.g., all were too short or too long).\n",
    "        return None, inferred_direction, original_text_for_context\n",
    "\n",
    "def is_valid_primer_sequence(sequence_bases, min_len=18, max_len=40):\n",
    "    if not isinstance(sequence_bases, str): return False \n",
    "    if not (min_len <= len(sequence_bases) <= max_len):\n",
    "        return False\n",
    "    if not all(char_val.upper() in ALLOWED_DNA_CHARS_FOR_FILTER for char_val in sequence_bases):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def clean_gene_name(gene_text):\n",
    "    if not isinstance(gene_text, str) or not gene_text.strip() or gene_text.lower() == 'nan':\n",
    "        return \"Unknown\"\n",
    "    match = re.match(r\"([a-zA-Z0-9_.\\-]+(?:[-/][a-zA-Z0-9_.\\-]+)*)\", gene_text.strip())\n",
    "    if match:\n",
    "        name = match.group(1)\n",
    "        if len(name) > 1 and not (name.isnumeric() and len(name) > 4): \n",
    "            # Convert common direction keywords found in gene cells to \"Unknown\" gene\n",
    "            # if they were mistakenly identified as gene name\n",
    "            if name.lower() not in (FORWARD_KEYWORDS + REVERSE_KEYWORDS + [\"sense\", \"antisense\", \"forward primer\", \"reverse primer\"]):\n",
    "                 # Avoid names that are purely sequence like\n",
    "                if not (len(name) > 15 and all(c.upper() in ALLOWED_DNA_CHARS_FOR_FILTER for c in name)):\n",
    "                    return name\n",
    "    return \"Unknown\" \n",
    "\n",
    "\n",
    "def find_primers_sequence_first(pdf_path, page_spec=\"all\"):\n",
    "    all_found_primers_details = []\n",
    "    pdf_filename = os.path.basename(pdf_path)\n",
    "\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: PDF file not found at {pdf_path}\")\n",
    "        return all_found_primers_details\n",
    "\n",
    "    print(f\"Attempting to read tables from '{pdf_filename}' pages: {page_spec}\")\n",
    "    \n",
    "    start_camelot_time = time.time()\n",
    "    lattice_tables_content = []\n",
    "    stream_tables_content = []\n",
    "\n",
    "    try:\n",
    "        tables_lattice_obj = camelot.read_pdf(pdf_path, pages=page_spec, flavor='lattice', line_scale=30, shift_text=[' '], copy_text=['v'], suppress_stdout=True)\n",
    "        if tables_lattice_obj.n > 0: \n",
    "            lattice_tables_content = list(tables_lattice_obj) \n",
    "    except Exception as e_lattice:\n",
    "        print(f\"  Lattice flavor failed for {pdf_filename}. Error: {e_lattice}\")\n",
    "\n",
    "    try:\n",
    "        tables_stream_obj = camelot.read_pdf(pdf_path, pages=page_spec, flavor='stream', suppress_stdout=True)\n",
    "        if tables_stream_obj.n > 0: \n",
    "            stream_tables_content = list(tables_stream_obj) \n",
    "    except Exception as e_stream:\n",
    "        print(f\"  Stream flavor failed for {pdf_filename}. Error: {e_stream}\")\n",
    "    \n",
    "    end_camelot_time = time.time()\n",
    "    print(f\"  Camelot PDF processing took: {end_camelot_time - start_camelot_time:.2f} seconds.\")\n",
    "        \n",
    "    combined_tables_list = lattice_tables_content + stream_tables_content\n",
    "        \n",
    "    if not combined_tables_list:\n",
    "        print(f\"No tables found by Camelot (lattice or stream combined) in {pdf_filename} on pages '{page_spec}'.\")\n",
    "        return all_found_primers_details\n",
    "    # --- End Timing for Camelot ---  \n",
    "    \n",
    "    start_python_processing_time = time.time()\n",
    "    unique_table_identifiers = set() \n",
    "\n",
    "    for table_idx, table_obj in enumerate(combined_tables_list): \n",
    "        try:\n",
    "            table_content_hash = hash(table_obj.df.to_string())\n",
    "            table_id = (table_obj.page, table_content_hash, table_obj.flavor) \n",
    "            if table_id in unique_table_identifiers: continue\n",
    "            unique_table_identifiers.add(table_id)\n",
    "        except Exception: pass\n",
    "\n",
    "        df = table_obj.df \n",
    "        # print(f\"\\n--- Scanning Table {table_idx+1} (Page {table_obj.page}, Shape {df.shape}, Flavor {table_obj.flavor}) ---\")\n",
    "\n",
    "        if df.empty or df.shape[0] < 1 or df.shape[1] < 1: continue\n",
    "\n",
    "        # Get column headers for context (from df.columns and potentially df.iloc[0])\n",
    "        headers_from_df_columns = [str(col).strip().lower() for col in df.columns] \n",
    "        first_row_values_as_headers = [str(df.iloc[0, c_idx]).strip().lower() for c_idx in range(min(df.shape[1], len(df.iloc[0])))] if df.shape[0] > 0 else []\n",
    "    \n",
    "        for r_idx in range(df.shape[0]):\n",
    "            # Attempt to determine a contextual gene for the entire row first\n",
    "            row_context_gene = \"Unknown\"\n",
    "            # Check first cell of the row for gene, if it's not a sequence itself\n",
    "            if df.shape[1] > 0:\n",
    "                first_cell_text = str(df.iloc[r_idx, 0]).strip()\n",
    "                if first_cell_text and first_cell_text.lower() != 'nan':\n",
    "                    temp_bases, _, _ = extract_sequence_and_direction_from_cell(first_cell_text)\n",
    "                    if not (temp_bases and is_valid_primer_sequence(temp_bases, min_len=15)):\n",
    "                        cleaned_name = clean_gene_name(first_cell_text)\n",
    "                        if cleaned_name != \"Unknown\": row_context_gene = cleaned_name\n",
    "            \n",
    "            for c_idx in range(df.shape[1]):\n",
    "                try:\n",
    "                    raw_cell_text = str(df.iloc[r_idx, c_idx]).strip()\n",
    "                    if not raw_cell_text or raw_cell_text.lower() == 'nan' or len(raw_cell_text) < 10: \n",
    "                        continue\n",
    "\n",
    "                    core_bases, direction_from_cell, original_cell_text = \\\n",
    "                        extract_sequence_and_direction_from_cell(raw_cell_text)\n",
    "\n",
    "                    if core_bases and is_valid_primer_sequence(core_bases):\n",
    "                        probable_gene = row_context_gene # Start with row-level gene context\n",
    "                        probable_direction = direction_from_cell if direction_from_cell else \"Unknown\"\n",
    "\n",
    "                        # If gene is still unknown from row context, try column to the left (if not first col)\n",
    "                        if probable_gene == \"Unknown\" and c_idx > 0:\n",
    "                            gene_cand_text = str(df.iloc[r_idx, c_idx - 1]).strip()\n",
    "                            if gene_cand_text and gene_cand_text.lower() != 'nan':\n",
    "                                temp_bases, _, _ = extract_sequence_and_direction_from_cell(gene_cand_text)\n",
    "                                if not (temp_bases and is_valid_primer_sequence(temp_bases, min_len=15)):\n",
    "                                    cleaned_name = clean_gene_name(gene_cand_text)\n",
    "                                    if cleaned_name != \"Unknown\": probable_gene = cleaned_name\n",
    "                        \n",
    "                        # Refine gene if current cell (which has primer) also has a gene-like prefix not caught by direction stripping\n",
    "                        if probable_gene == \"Unknown\" or len(probable_gene) < 2 : # If context gene weak or unknown\n",
    "                            gene_part_from_primer_cell_match = re.match(r\"([a-zA-Z0-9_.\\-]+)\\s*[:\\-(]\", original_cell_text)\n",
    "                            if gene_part_from_primer_cell_match:\n",
    "                                potential_gene_prefix = clean_gene_name(gene_part_from_primer_cell_match.group(1))\n",
    "                                if potential_gene_prefix != \"Unknown\":\n",
    "                                    probable_gene = potential_gene_prefix\n",
    "\n",
    "\n",
    "                        # Attempt to find Direction from column header if not found in cell:\n",
    "                        if probable_direction == \"Unknown\":\n",
    "                            current_col_header = \"\"\n",
    "                            # Prioritize Camelot's direct column name if it's text\n",
    "                            if c_idx < len(headers_from_df_columns) and not all(isinstance(c, int) for c in df.columns):\n",
    "                                current_col_header = headers_from_df_columns[c_idx]\n",
    "                            # Else, if columns were int, first row might be header\n",
    "                            elif c_idx < len(first_row_values_as_headers): \n",
    "                                current_col_header = first_row_values_as_headers[c_idx]\n",
    "\n",
    "                            if current_col_header: \n",
    "                                if any(k in current_col_header for k in FORWARD_KEYWORDS): probable_direction = \"Forward\"\n",
    "                                elif any(k in current_col_header for k in REVERSE_KEYWORDS): probable_direction = \"Reverse\"\n",
    "                        \n",
    "                        # Final fallback for direction if sequence is in one cell, and gene name is in cell above, \n",
    "                        # and direction keywords are beside gene name in cell above\n",
    "                        if probable_direction == \"Unknown\" and r_idx > 0 and probable_gene != \"Unknown\":\n",
    "                            cell_above_text = str(df.iloc[r_idx-1, c_idx]).strip().lower()\n",
    "                            if probable_gene.lower() in cell_above_text: # if gene name is part of cell above\n",
    "                                if any(k in cell_above_text for k in FORWARD_KEYWORDS): probable_direction = \"Forward\"\n",
    "                                elif any(k in cell_above_text for k in REVERSE_KEYWORDS): probable_direction = \"Reverse\"\n",
    "\n",
    "\n",
    "                        all_found_primers_details.append({\n",
    "                            \"Sequence\": core_bases, \"Probable Gene\": probable_gene, \n",
    "                            \"Probable Direction\": probable_direction, \"Source File\": pdf_filename, \n",
    "                            \"Page\": table_obj.page, \"Original Cell Text\": original_cell_text\n",
    "                        })\n",
    "                except Exception: continue\n",
    "\n",
    "    \n",
    "     # --- Transform to new output format (with .get() for safety) ---\n",
    "    final_results_transformed = []\n",
    "    for p_info in all_found_primers_details:\n",
    "        # Use .get() to provide default values if a key is somehow missing from p_info\n",
    "        source_file = p_info.get(\"Source File\", pdf_filename) # Fallback to current pdf_filename\n",
    "        probable_gene = p_info.get(\"Probable Gene\", \"Unknown\")\n",
    "        sequence_val = p_info.get(\"Sequence\", \"\") # Must have sequence\n",
    "        probable_direction = p_info.get(\"Probable Direction\", \"Unknown\")\n",
    "        page_val = p_info.get(\"Page\", 0) # Default page to 0 if missing\n",
    "        original_text = p_info.get(\"Original Cell Text\", \"\")\n",
    "\n",
    "        if not sequence_val: # Skip if no sequence was extracted for this entry\n",
    "            continue\n",
    "\n",
    "        pmcid = extract_pmcid_from_filename(source_file)\n",
    "        \n",
    "        orientation = \"Unknown\"\n",
    "        if probable_direction: \n",
    "            direction_lower = probable_direction.lower()\n",
    "            if \"forward\" in direction_lower or \"sense\" in direction_lower:\n",
    "                orientation = \"Forward\"\n",
    "            elif \"reverse\" in direction_lower or \"antisense\" in direction_lower:\n",
    "                orientation = \"Reverse\"\n",
    "\n",
    "        final_results_transformed.append({\n",
    "            \"PMCID\": pmcid,\n",
    "            \"Gene\": probable_gene,\n",
    "            \"Sequence\": sequence_val,\n",
    "            \"Orientation\": orientation,\n",
    "            \"Source File\": source_file,\n",
    "            \"Page\": page_val,\n",
    "            \"Original Cell Text\": original_text\n",
    "        })\n",
    "    \n",
    "    end_python_processing_time = time.time()\n",
    "    # print(f\"  Python post-Camelot processing took: {end_python_processing_time - start_python_processing_time:.2f} seconds.\")\n",
    "            \n",
    "    return final_results_transformed\n",
    "\n",
    "# Add this function definition with your other helper functions\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 6.  main text reader functions  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-\n",
    "# -----------------------------------------------------------------------------------\n",
    "def process_pdf_with_timeout_wrapper(args_tuple):\n",
    "    \"\"\"\n",
    "    Wrapper function to call find_primers_sequence_first for a single PDF.\n",
    "    Handles arguments and exceptions for multiprocessing.\n",
    "    \"\"\"\n",
    "    pdf_path, page_spec, hgnc_symbols_set, pdf_filename_for_log = args_tuple\n",
    "    # print(f\"  Worker starting for: {pdf_filename_for_log}, pages: {page_spec}\") # Optional: for verbose worker start\n",
    "    try:\n",
    "        result = find_primers_sequence_first(pdf_path, page_spec, hgnc_symbols_set)\n",
    "        # print(f\"  Worker finished for: {pdf_filename_for_log}. Found {len(result)} primer items.\") # Optional\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR in worker process for {pdf_filename_for_log}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback from worker for debugging\n",
    "        return [] # Return empty list on error within the worker\n",
    "        \n",
    "def get_element_text_content(element) -> str:\n",
    "    \"\"\"Extracts and concatenates all text within an XML element.\"\"\"\n",
    "    return \" \".join(text.strip() for text in element.xpath(\".//text()\") if text.strip()).strip()\n",
    "def parse_xml_table_to_dataframe(table_etree_element):\n",
    "    \"\"\"Converts an lxml etree <table> element to a pandas DataFrame.\"\"\"\n",
    "    headers = []\n",
    "    header_elements = table_etree_element.xpath(\"./thead/tr/th | ./tr[1]/th | ./thead/tr/td | ./tr[1]/td\") # More comprehensive header find\n",
    "    \n",
    "    # If the first row has only <th>, it's likely the header row\n",
    "    if header_elements and all(el.tag == 'th' for el in table_etree_element.xpath(\"./tr[1]/*\")):\n",
    "         headers = [get_element_text_content(th).strip() for th in header_elements]\n",
    "    elif header_elements : # if first row has mix of th/td or just td but are headers from xpath\n",
    "         headers = [get_element_text_content(th).strip() for th in header_elements]\n",
    "\n",
    "\n",
    "    rows_data = []\n",
    "    # Get all table rows (tr)\n",
    "    tr_elements = table_etree_element.xpath(\"./tbody/tr | ./tr\") # Get body rows or all rows if no tbody\n",
    "    \n",
    "    start_row_index = 0\n",
    "    if headers and len(tr_elements) > 0 : # If headers were found from tr[1] or thead\n",
    "        # Check if the first tr_element was used for headers\n",
    "        first_tr_cells_for_header_check = tr_elements[0].xpath(\"./th | ./td\")\n",
    "        first_tr_text_for_header_check = [get_element_text_content(c).strip() for c in first_tr_cells_for_header_check]\n",
    "        if first_tr_text_for_header_check == headers:\n",
    "            start_row_index = 1 # Data rows start from the next row\n",
    "\n",
    "    for r_idx in range(start_row_index, len(tr_elements)):\n",
    "        tr = tr_elements[r_idx]\n",
    "        cells = tr.xpath(\"./td | ./th\") # Cells can be td or th (e.g. row headers)\n",
    "        row_text = [get_element_text_content(cell).strip() for cell in cells]\n",
    "        if any(rt.strip() for rt in row_text): # Only add non-empty rows\n",
    "            rows_data.append(row_text)\n",
    "\n",
    "    if not rows_data:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(rows_data)\n",
    "    if headers and len(headers) == df.shape[1]:\n",
    "        df.columns = headers\n",
    "    elif df.shape[0] > 0 and not headers: # No explicit headers found, try to use first data row if suitable\n",
    "        first_row_is_data_like = any(is_valid_primer_sequence(extract_sequence_and_direction_from_cell(str(c))[0] or \"\", 18,40) for c in df.iloc[0])\n",
    "        if not first_row_is_data_like and df.shape[0] > 1 :\n",
    "            df.columns = [str(c).strip() for c in df.iloc[0]]\n",
    "            df = df[1:].reset_index(drop=True)\n",
    "        # Else: use default integer column names from pandas\n",
    "    return df\n",
    "\n",
    "def scan_dataframe_for_primers(df: pd.DataFrame, pmcid: str, \n",
    "                               source_description: str, # e.g., \"XML Table in file.xml\" or \"PDF Page X\"\n",
    "                               source_filename_context: str, # filename for XML, or page number for PDF\n",
    "                               hgnc_symbols: set) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scans a given DataFrame (from PDF or XML table) for primer sequences.\n",
    "    \"\"\"\n",
    "    found_primers_in_df = []\n",
    "    if df.empty:\n",
    "        return found_primers_in_df\n",
    "\n",
    "    df_headers_original_case = [str(col).strip() for col in df.columns] # Keep original case for display/debug if needed\n",
    "    df_headers_lower = [h.lower() for h in df_headers_original_case]\n",
    "\n",
    "    for r_idx in range(df.shape[0]):\n",
    "        row_context_gene = \"Unknown\"\n",
    "        # Try to get row context gene from the first cell if it's text-like and not a sequence\n",
    "        if df.shape[1] > 0:\n",
    "            first_cell_text_for_gene = str(df.iloc[r_idx, 0]).strip()\n",
    "            if first_cell_text_for_gene and first_cell_text_for_gene.lower() != 'nan':\n",
    "                temp_bases_gene, _, _ = extract_sequence_and_direction_from_cell(first_cell_text_for_gene)\n",
    "                # A gene name shouldn't be a long valid primer sequence\n",
    "                if not (temp_bases_gene and is_valid_primer_sequence(temp_bases_gene, min_len=15)): # Check if it's NOT a primer\n",
    "                    cleaned_name = clean_gene_name(first_cell_text_for_gene) # clean_gene_name does further checks\n",
    "                    if cleaned_name != \"Unknown\":\n",
    "                         # Validate against HGNC if it looks like a gene symbol\n",
    "                        if cleaned_name.upper() in hgnc_symbols or (len(cleaned_name) > 4 and not any(k in cleaned_name.lower() for k in FORWARD_KEYWORDS + REVERSE_KEYWORDS)): # Allow longer non-HGNC names\n",
    "                            row_context_gene = cleaned_name\n",
    "\n",
    "        for c_idx in range(df.shape[1]):\n",
    "            try:\n",
    "                raw_cell_text = str(df.iloc[r_idx, c_idx]).strip()\n",
    "                if not raw_cell_text or raw_cell_text.lower() == 'nan' or len(raw_cell_text) < 10:\n",
    "                    continue\n",
    "\n",
    "                core_bases, direction_from_cell, original_cell_text = \\\n",
    "                    extract_sequence_and_direction_from_cell(raw_cell_text)\n",
    "\n",
    "                if core_bases and is_valid_primer_sequence(core_bases):\n",
    "                    sequence = core_bases\n",
    "                    probable_direction = direction_from_cell if direction_from_cell else \"Unknown\"\n",
    "                    probable_gene = row_context_gene # Start with row context\n",
    "\n",
    "                    # If gene still \"Unknown\" from row context, try column to the left\n",
    "                    if probable_gene == \"Unknown\" and c_idx > 0:\n",
    "                        gene_cand_text_left = str(df.iloc[r_idx, c_idx - 1]).strip()\n",
    "                        if gene_cand_text_left and gene_cand_text_left.lower() != 'nan':\n",
    "                            temp_bases_left, _, _ = extract_sequence_and_direction_from_cell(gene_cand_text_left)\n",
    "                            if not (temp_bases_left and is_valid_primer_sequence(temp_bases_left, min_len=15)):\n",
    "                                cleaned_name_left = clean_gene_name(gene_cand_text_left)\n",
    "                                if cleaned_name_left != \"Unknown\": probable_gene = cleaned_name_left\n",
    "                    \n",
    "                    # If gene is still \"Unknown\" and current cell has gene-like prefix before primer\n",
    "                    if probable_gene == \"Unknown\" or len(probable_gene) < 2:\n",
    "                        gene_part_match = re.match(r\"([a-zA-Z0-9_.\\-]+(?:[-/][a-zA-Z0-9_.\\-]+)*)\\s*[:\\-(]\", original_cell_text)\n",
    "                        if gene_part_match:\n",
    "                            potential_gene_prefix = clean_gene_name(gene_part_match.group(1))\n",
    "                            if potential_gene_prefix != \"Unknown\": probable_gene = potential_gene_prefix\n",
    "                    \n",
    "                    # Use column header for direction if not found in cell\n",
    "                    if probable_direction == \"Unknown\" and c_idx < len(df_headers_lower):\n",
    "                        col_header_text = df_headers_lower[c_idx]\n",
    "                        if any(k in col_header_text for k in FORWARD_KEYWORDS):\n",
    "                            probable_direction = \"Forward\"\n",
    "                        elif any(k in col_header_text for k in REVERSE_KEYWORDS):\n",
    "                            probable_direction = \"Reverse\"\n",
    "                    \n",
    "                    # If gene is still \"Unknown\", check current column header (if not a direction/sequence keyword itself)\n",
    "                    if probable_gene == \"Unknown\" and c_idx < len(df_headers_lower):\n",
    "                        header_text = df_headers_lower[c_idx]\n",
    "                        is_dir_header = any(k in header_text for k in FORWARD_KEYWORDS + REVERSE_KEYWORDS)\n",
    "                        is_seq_header = any(k in header_text for k in ['sequence', 'primer', 'oligo', 'probe'])\n",
    "                        if not is_dir_header and not is_seq_header and len(df_headers_original_case[c_idx]) > 1 : # Use original case for clean_gene_name\n",
    "                            cleaned_header_gene = clean_gene_name(df_headers_original_case[c_idx])\n",
    "                            if cleaned_header_gene != \"Unknown\": probable_gene = cleaned_header_gene\n",
    "\n",
    "                    found_primers_in_df.append({\n",
    "                        \"PMCID\": pmcid, \"Gene\": probable_gene, \"Sequence\": sequence,\n",
    "                        \"Orientation\": probable_direction, \"Source File\": source_filename_context, # Use the context string\n",
    "                        \"Page\": source_description, # General description (e.g. \"XML Table\", \"PDF Page X\")\n",
    "                        \"Original Cell Text\": original_cell_text\n",
    "                    })\n",
    "            except Exception: # Catch errors within cell processing\n",
    "                continue\n",
    "    return found_primers_in_df\n",
    "\n",
    "\n",
    "def extract_primers_from_xml(xml_path: Path, pmcid: str, hgnc_symbols: set) -> List[Dict]:\n",
    "    found_xml_primers = []\n",
    "    if not xml_path.exists(): return found_xml_primers\n",
    "\n",
    "    try:\n",
    "        with open(xml_path, 'rb') as f: xml_content_bytes = f.read()\n",
    "        parser = etree.XMLParser(recover=True, strip_cdata=True, resolve_entities=False, no_network=True)\n",
    "        root = etree.fromstring(xml_content_bytes, parser=parser)\n",
    "    except Exception as e:\n",
    "        print(f\"  Error parsing XML for {pmcid} ({xml_path.name}): {e}\")\n",
    "        return found_xml_primers\n",
    "\n",
    "    # --- 1. Process Tables within XML ---\n",
    "    table_elements = root.xpath(\"//table[ancestor::table-wrap]\") # More specific: tables within table-wraps\n",
    "    if not table_elements: # Fallback to any table element\n",
    "        table_elements = root.xpath(\"//table\")\n",
    "    \n",
    "    print(f\"  Found {len(table_elements)} <table> elements in XML for {pmcid}.\")\n",
    "    for idx, table_el in enumerate(table_elements):\n",
    "        # print(f\"    Processing XML Table {idx+1} in {pmcid}...\")\n",
    "        df_from_xml = parse_xml_table_to_dataframe(table_el)\n",
    "        if not df_from_xml.empty:\n",
    "            source_desc = f\"XML Table {idx+1} (cols: {list(df_from_xml.columns)})\"\n",
    "            primers_from_table = scan_dataframe_for_primers(df_from_xml, pmcid, source_desc, xml_path.name, hgnc_symbols)\n",
    "            if primers_from_table:\n",
    "                found_xml_primers.extend(primers_from_table)\n",
    "        # else:\n",
    "            # print(f\"      XML Table {idx+1} parsed to empty DataFrame.\")\n",
    "\n",
    "    # --- 2. Process Prose Text ---\n",
    "    # Define XPaths for prose content, trying to exclude table cell content already processed\n",
    "    prose_xpaths = [\n",
    "        \"//sec[translate(@sec-type,'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz')='methods']//p[not(.//table)]\",\n",
    "        \"//sec[translate(@sec-type,'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz')='materialsmethods']//p[not(.//table)]\",\n",
    "        \"//body//p[not(ancestor::table-wrap) and not(ancestor::fig) and not(.//table)]\",\n",
    "        \"//fig//caption//p[not(.//table)]\",\n",
    "        \"//table-wrap//caption//p[not(.//table)]\", # Caption of table-wrap if it doesn't contain a table itself\n",
    "        \"//table-wrap-foot//p[not(.//table)]\"\n",
    "    ]\n",
    "    unique_prose_blocks = set()\n",
    "    dna_candidate_regex = re.compile(r'\\b((?:[ACGT]\\s*){15,50})\\b', re.IGNORECASE)\n",
    "\n",
    "    for xp in prose_xpaths:\n",
    "        try:\n",
    "            elements = root.xpath(xp)\n",
    "            for elem in elements:\n",
    "                text_block = get_element_text_content(elem)\n",
    "                if not text_block or len(text_block) < 20 or text_block in unique_prose_blocks:\n",
    "                    continue\n",
    "                unique_prose_blocks.add(text_block)\n",
    "\n",
    "                for match in dna_candidate_regex.finditer(text_block):\n",
    "                    raw_dna_match_with_spaces = match.group(1)\n",
    "                    core_bases, direction_from_text_body, _ = \\\n",
    "                        extract_sequence_and_direction_from_cell(raw_dna_match_with_spaces) # Use this to clean and get initial direction\n",
    "\n",
    "                    if core_bases and is_valid_primer_sequence(core_bases):\n",
    "                        sequence = core_bases\n",
    "                        probable_direction = direction_from_text_body if direction_from_text_body else \"Unknown\"\n",
    "                        probable_gene = \"Unknown\"\n",
    "                        \n",
    "                        match_start_in_block = match.start()\n",
    "                        match_end_in_block = match.end()\n",
    "                        \n",
    "                        # Context for direction and gene (search around the raw_dna_match_with_spaces)\n",
    "                        context_chars = 80 # How many chars before/after for context\n",
    "                        text_before = text_block[max(0, match_start_in_block - context_chars) : match_start_in_block]\n",
    "                        text_after = text_block[match_end_in_block : min(len(text_block), match_end_in_block + context_chars)]\n",
    "                        \n",
    "                        # Refine direction from local context if not found by extract_sequence_and_direction_from_cell\n",
    "                        if probable_direction == \"Unknown\":\n",
    "                            if any(k in text_before.lower() or k in text_after.lower() for k in FORWARD_KEYWORDS):\n",
    "                                probable_direction = \"Forward\"\n",
    "                            elif any(k in text_before.lower() or k in text_after.lower() for k in REVERSE_KEYWORDS):\n",
    "                                probable_direction = \"Reverse\"\n",
    "                        \n",
    "                        # Gene finding in prose (simplified example, can be improved)\n",
    "                        # Look for capitalized words or HGNC symbols near the primer\n",
    "                        gene_search_context = text_before + raw_dna_match_with_spaces + text_after\n",
    "                        potential_genes = re.findall(r'\\b([A-Z][A-Za-z0-9-]{2,15})\\b', gene_search_context) # Find capitalized words\n",
    "                        best_dist = float('inf')\n",
    "                        \n",
    "                        common_prose_noise = {\"PCR\", \"DNA\", \"RNA\", \"PRIMER\", \"FORWARD\", \"REVERSE\", \"SEQUENCE\", \"METHOD\", \"FIGURE\", \"TABLE\"} | set(k.upper() for k in FORWARD_KEYWORDS) | set(k.upper() for k in REVERSE_KEYWORDS)\n",
    "\n",
    "                        for pg_cand in potential_genes:\n",
    "                            pg_upper = pg_cand.upper()\n",
    "                            if pg_upper in hgnc_symbols and pg_upper not in common_prose_noise:\n",
    "                                # Find position of pg_cand relative to raw_dna_match_with_spaces in gene_search_context\n",
    "                                for m_pg in re.finditer(re.escape(pg_cand), gene_search_context, re.IGNORECASE):\n",
    "                                    # Relative start of primer in gene_search_context\n",
    "                                    primer_rel_start = len(text_before) \n",
    "                                    dist = abs(m_pg.start() - primer_rel_start)\n",
    "                                    if dist < best_dist and dist < context_chars : # Must be close\n",
    "                                        best_dist = dist\n",
    "                                        probable_gene = clean_gene_name(pg_cand)\n",
    "                        \n",
    "                        original_context_snip = f\"...{text_block[max(0,match_start_in_block-30):min(len(text_block),match_end_in_block+30)]}...\"\n",
    "                        found_xml_primers.append({\n",
    "                            \"PMCID\": pmcid, \"Gene\": probable_gene, \"Sequence\": sequence,\n",
    "                            \"Orientation\": probable_direction, \"Source File\": xml_path.name, \n",
    "                            \"Page\": \"Main Text Prose\", \"Original Cell Text\": original_context_snip\n",
    "                        })\n",
    "        except Exception: continue # Ignore errors for a single XPath or element\n",
    "            \n",
    "    return found_xml_primers\n",
    "\n",
    "\n",
    "def find_primers_sequence_first(pdf_path, page_spec=\"all\", hgnc_symbols: set = set()):\n",
    "    all_found_primers_details = [] # This will store dicts from scan_dataframe_for_primers\n",
    "    pdf_filename = os.path.basename(pdf_path)\n",
    "    \n",
    "    # Timeout for each Camelot call (lattice and stream)\n",
    "    CAMELOT_CALL_TIMEOUT = 60  # Seconds (e.g., 1.5 minutes per Camelot flavor call)\n",
    "\n",
    "    # print(f\"Attempting to read tables from '{pdf_filename}' pages: {page_spec}\") # Already printed in main\n",
    "    \n",
    "    start_camelot_time = time.time()\n",
    "    lattice_tables_content = []\n",
    "    stream_tables_content = []\n",
    "\n",
    "    # Camelot parameters\n",
    "    camelot_params_lattice = {\n",
    "        'pages': page_spec, \n",
    "        'flavor': 'lattice', \n",
    "        'line_scale': 30, \n",
    "        'shift_text': [' '], \n",
    "        'copy_text': ['v'], \n",
    "        'suppress_stdout': True\n",
    "    }\n",
    "    camelot_params_stream = {\n",
    "        'pages': page_spec, \n",
    "        'flavor': 'stream', \n",
    "        'suppress_stdout': True\n",
    "    }\n",
    "\n",
    "    with ProcessPool(max_workers=1) as pool: # Using 1 worker as we do calls sequentially\n",
    "        # Lattice\n",
    "        try:\n",
    "            # print(f\"  Submitting 'lattice' task for {pdf_filename}...\")\n",
    "            future_lattice = pool.schedule(camelot.read_pdf, args=[pdf_path], kwargs=camelot_params_lattice, timeout=CAMELOT_CALL_TIMEOUT)\n",
    "            tables_lattice_obj = future_lattice.result()  # Blocks until result or timeout\n",
    "            if tables_lattice_obj and tables_lattice_obj.n > 0:\n",
    "                lattice_tables_content = list(tables_lattice_obj)\n",
    "            # print(f\"  Lattice found {len(lattice_tables_content)} table(s).\")\n",
    "        except PebbleTimeoutError:\n",
    "            print(f\"  Lattice flavor TIMED OUT for {pdf_filename} after {CAMELOT_CALL_TIMEOUT}s on pages '{page_spec}'.\")\n",
    "        except Exception as e_lattice:\n",
    "            print(f\"  Lattice flavor failed for {pdf_filename} (pages '{page_spec}'). Error: {e_lattice}\")\n",
    "\n",
    "        # Stream\n",
    "        try:\n",
    "            # print(f\"  Submitting 'stream' task for {pdf_filename}...\")\n",
    "            future_stream = pool.schedule(camelot.read_pdf, args=[pdf_path], kwargs=camelot_params_stream, timeout=CAMELOT_CALL_TIMEOUT)\n",
    "            tables_stream_obj = future_stream.result() # Blocks until result or timeout\n",
    "            if tables_stream_obj and tables_stream_obj.n > 0:\n",
    "                stream_tables_content = list(tables_stream_obj)\n",
    "            # print(f\"  Stream found {len(stream_tables_content)} table(s).\")\n",
    "        except PebbleTimeoutError:\n",
    "            print(f\"  Stream flavor TIMED OUT for {pdf_filename} after {CAMELOT_CALL_TIMEOUT}s on pages '{page_spec}'.\")\n",
    "        except Exception as e_stream:\n",
    "            print(f\"  Stream flavor failed for {pdf_filename} (pages '{page_spec}'). Error: {e_stream}\")\n",
    "    \n",
    "    end_camelot_time = time.time()\n",
    "    print(f\"  Camelot PDF processing for pages '{page_spec}' took: {end_camelot_time - start_camelot_time:.2f} seconds (including any timeouts).\")\n",
    "        \n",
    "    combined_tables_list = lattice_tables_content + stream_tables_content\n",
    "        \n",
    "    if not combined_tables_list:\n",
    "        # print(f\"  No tables found by Camelot in {pdf_filename} on pages '{page_spec}'.\") # Already handled by individual prints\n",
    "        return [] # Return empty list, not all_found_primers_details which isn't defined yet in this scope\n",
    "        \n",
    "    # print(f\"  Found {len(combined_tables_list)} potential table regions in {pdf_filename}.\")\n",
    "\n",
    "    start_python_processing_time = time.time()\n",
    "    # --- (The rest of your table scanning, cell iteration, and context gathering logic) ---\n",
    "    # This part iterates `combined_tables_list` and calls `scan_dataframe_for_primers`\n",
    "    # or directly processes df from table_obj to populate `all_found_primers_details`\n",
    "    \n",
    "    unique_table_identifiers = set()\n",
    "    for table_idx, table_obj in enumerate(combined_tables_list):\n",
    "        try:\n",
    "            table_content_hash = hash(table_obj.df.to_string())\n",
    "            table_id = (table_obj.page, table_content_hash, table_obj.flavor)\n",
    "            if table_id in unique_table_identifiers: continue\n",
    "            unique_table_identifiers.add(table_id)\n",
    "        except Exception: pass\n",
    "        \n",
    "        df_from_pdf = table_obj.df\n",
    "        if df_from_pdf.empty: continue\n",
    "        \n",
    "        source_desc = f\"PDF Page {table_obj.page}, Table {table_idx+1} ({table_obj.flavor})\"\n",
    "        # Extract PMCID from the PDF filename to pass to scan_dataframe_for_primers\n",
    "        current_pmcid = extract_pmcid_from_filename(pdf_filename)\n",
    "\n",
    "        # Ensure scan_dataframe_for_primers is defined and accessible\n",
    "        primers_from_pdf_table = scan_dataframe_for_primers(\n",
    "            df_from_pdf, \n",
    "            current_pmcid, \n",
    "            source_desc,\n",
    "            pdf_filename, # filename_or_context\n",
    "            hgnc_symbols\n",
    "        )\n",
    "        if primers_from_pdf_table:\n",
    "            all_found_primers_details.extend(primers_from_pdf_table)\n",
    "\n",
    "    end_python_processing_time = time.time()\n",
    "    # print(f\"  Python post-Camelot processing took: {end_python_processing_time - start_python_processing_time:.2f} seconds.\")\n",
    "            \n",
    "    # The `all_found_primers_details` list contains dictionaries already in the desired final format\n",
    "    # as produced by `scan_dataframe_for_primers`.\n",
    "    # No further transformation loop is strictly needed here if `scan_dataframe_for_primers`\n",
    "    # correctly sets all required keys (\"PMCID\", \"Gene\", \"Orientation\", etc.).\n",
    "    # The error you had previously (KeyError: 'Probable Gene') would be in the loop below if it existed.\n",
    "    # The `scan_dataframe_for_primers` should be outputting with \"Gene\" and \"Orientation\".\n",
    "    \n",
    "    # For clarity, let's assume scan_dataframe_for_primers returns the final desired structure:\n",
    "    # {\"PMCID\": ..., \"Gene\": ..., \"Sequence\": ..., \"Orientation\": ..., \"Source File\": ..., \"Page\": ..., \"Original Cell Text\": ...}\n",
    "    \n",
    "    return all_found_primers_details\n",
    "\n",
    "\n",
    "# ###################################################################################\n",
    "# # --- (Main Execution Block - if __name__ == \"__main__\": ...) ---\n",
    "# ###################################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # --- 0. Configuration ---\n",
    "    records_to_fetch = 500 # Example from your code\n",
    "    START_YEAR = 2025\n",
    "    END_YEAR_PROCESSING = 2015 # Example from your code\n",
    "    N_LAST_PAGES = 5 # Used for PDF page targeting\n",
    "\n",
    "    print(\"Loading HGNC symbols...\")\n",
    "    HGNC = load_hgnc_symbol_set()\n",
    "    # xml_paths = [] # This isn't strictly necessary if processing XML immediately\n",
    "\n",
    "    # --- Main Year Loop ---\n",
    "    for year_to_process in range(START_YEAR, END_YEAR_PROCESSING - 1, -1): # Use year_to_process\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"STARTING PROCESSING FOR YEAR: {year_to_process}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        master_primer_list_for_year = [] # Initialize for the current year's results\n",
    "\n",
    "        # --- 1. Define year-specific query for Europe PMC ---\n",
    "        base_query = 'OPEN_ACCESS:y AND HAS_FT:y AND (METHODS:\"qPCR\" OR \"RT-PCR\" OR \"real time PCR\") AND (ABSTRACT:\"pluripotent\" OR \"iPSC\" OR \"PSC\" OR \"hPSC\" OR \"ESC\" OR \"hESC\")'\n",
    "        query = f\"{base_query} AND (FIRST_PDATE:{year_to_process})\"\n",
    "        print(f\"Searching Europe PMC with query: {query}\")\n",
    "        \n",
    "        epmc_hits_for_year = []\n",
    "        try:\n",
    "            epmc_hits_for_year = europepmc_search_all(query, max_records=records_to_fetch)\n",
    "            print(f\"Retrieved {len(epmc_hits_for_year)} records for {year_to_process}.\")\n",
    "        except Exception as e_search:\n",
    "            print(f\"Could not complete search for year {year_to_process}. Error: {e_search}\")\n",
    "            continue\n",
    "\n",
    "        if not epmc_hits_for_year:\n",
    "            print(f\"No records found for {year_to_process}. Skipping to next year.\")\n",
    "            continue\n",
    "\n",
    "        # --- 2. Process each paper for this year ---\n",
    "        for hit_index, hit in enumerate(tqdm(epmc_hits_for_year, desc=f\"Processing Papers for {year_to_process}\")):\n",
    "            current_pmcid_str = None\n",
    "            if isinstance(hit, dict):\n",
    "                current_pmcid_str = hit.get(\"pmcid\") or hit.get(\"id\")\n",
    "            else:\n",
    "                current_pmcid_str = str(hit)\n",
    "            \n",
    "            if not (current_pmcid_str and current_pmcid_str.startswith(\"PMC\")):\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n  Processing PMCID: {current_pmcid_str} ({hit_index+1}/{len(epmc_hits_for_year)}) for year {year_to_process}\")\n",
    "            \n",
    "            primers_found_for_this_pmcid_count = 0 # Initialize for THIS PMCID\n",
    "\n",
    "            try:\n",
    "                xml_file_for_current_paper = get_or_download_xml(current_pmcid_str, DATA_DIR)\n",
    "                \n",
    "                # --- 2c. Read primers from main text XML ---\n",
    "                if xml_file_for_current_paper.exists():\n",
    "                    # print(f\"    Extracting primers from XML: {xml_file_for_current_paper.name}\")\n",
    "                    xml_primers = extract_primers_from_xml(xml_file_for_current_paper, current_pmcid_str, HGNC)\n",
    "                    if xml_primers:\n",
    "                        master_primer_list_for_year.extend(xml_primers)\n",
    "                        primers_found_for_this_pmcid_count += len(xml_primers)\n",
    "                    # print(f\"      Found {len(xml_primers)} primer items from XML. Total for {current_pmcid_str}: {primers_found_for_this_pmcid_count}\")\n",
    "\n",
    "                # --- 2d. Fetch, Rename, and Process Supplements for THIS PMCID ---\n",
    "                renamed_supp_files_for_this_pmcid = [] # Initialize to ensure it's always defined\n",
    "\n",
    "                if primers_found_for_this_pmcid_count > 8:\n",
    "                    print(f\"      Sufficient primers ({primers_found_for_this_pmcid_count}) found for {current_pmcid_str} from XML. Skipping its supplement processing.\")\n",
    "                else:\n",
    "                    # print(f\"    Fetching supplements for {current_pmcid_str} (XML found: {primers_found_for_this_pmcid_count})...\")\n",
    "                    downloaded_supp_files = fetch_supplements_from_xml(current_pmcid_str, xml_file_for_current_paper, DATA_DIR / \"supp\")\n",
    "                    \n",
    "                    if downloaded_supp_files:\n",
    "                        renamed_supp_files_for_this_pmcid = rename_supp_files(current_pmcid_str, downloaded_supp_files)\n",
    "                        # print(f\"      â†³ {len(renamed_supp_files_for_this_pmcid)} supplement(s) to process for {current_pmcid_str}.\")\n",
    "                    # else:\n",
    "                        # print(f\"      No supplements found or downloaded for {current_pmcid_str}.\")\n",
    "\n",
    "                # Process the collected supplements for THIS PMCID\n",
    "                if renamed_supp_files_for_this_pmcid: # Only loop if there are supplements and we haven't skipped due to XML\n",
    "                    for supp_file_path_obj in tqdm(renamed_supp_files_for_this_pmcid, desc=f\"Supps for {current_pmcid_str}\", leave=False):\n",
    "                        if primers_found_for_this_pmcid_count > 8:\n",
    "                            print(f\"      Sufficient primers ({primers_found_for_this_pmcid_count}) now found for {current_pmcid_str}. Skipping its remaining supplements.\")\n",
    "                            break # Correctly breaks from THIS PMCID's supplement loop\n",
    "\n",
    "                        supp_file_path_str = str(supp_file_path_obj)\n",
    "                        supp_filename_for_log = os.path.basename(supp_file_path_str)\n",
    "                        # print(f\"\\n        Processing Supplement File: {supp_filename_for_log}\")\n",
    "                        \n",
    "                        extracted_supp_primers = []\n",
    "                        page_spec_for_camelot = \"all\" \n",
    "                        process_this_file_fully = True # Renamed from process_this_pdf_fully\n",
    "\n",
    "                        # Determine page_spec and if it's a reporting summary (for PDFs)\n",
    "                        if supp_filename_for_log.lower().endswith(\".pdf\"):\n",
    "                            try:\n",
    "                                with pdfplumber.open(supp_file_path_str) as pdf_doc:\n",
    "                                    if not pdf_doc.pages:\n",
    "                                        process_this_file_fully = False\n",
    "                                    else:\n",
    "                                        total_pages = len(pdf_doc.pages)\n",
    "                                        first_page_text = pdf_doc.pages[0].extract_text() if pdf_doc.pages[0] else \"\"\n",
    "                                        if first_page_text and \"reporting summary\" in first_page_text.lower():\n",
    "                                            page_spec_for_camelot = \"1\"\n",
    "                                        elif total_pages == 0: page_spec_for_camelot = \"1\"\n",
    "                                        elif total_pages <= N_LAST_PAGES: page_spec_for_camelot = f\"1-{total_pages}\"\n",
    "                                        else:\n",
    "                                            start_page = total_pages - N_LAST_PAGES + 1\n",
    "                                            page_spec_for_camelot = f\"{start_page}-{total_pages}\"\n",
    "                            except Exception as e_pg_cnt:\n",
    "                                # print(f\"        Error getting page count for {supp_filename_for_log}: {e_pg_cnt}. Defaulting to all pages.\")\n",
    "                                page_spec_for_camelot = \"all\" # Keep process_this_file_fully as True to attempt parsing\n",
    "\n",
    "                        # Process the file based on its type\n",
    "                        if process_this_file_fully:\n",
    "                            if supp_filename_for_log.lower().endswith(\".pdf\"):\n",
    "                                extracted_supp_primers = find_primers_sequence_first(supp_file_path_str, page_spec=page_spec_for_camelot, hgnc_symbols=HGNC)\n",
    "                            elif supp_filename_for_log.lower().endswith(\".docx\"):\n",
    "                                extracted_supp_primers = extract_primers_from_docx(supp_file_path_obj, current_pmcid_str, HGNC)\n",
    "                            # Add more elif for other file types if needed\n",
    "\n",
    "                        if extracted_supp_primers:\n",
    "                            master_primer_list_for_year.extend(extracted_supp_primers)\n",
    "                            primers_found_for_this_pmcid_count += len(extracted_supp_primers)\n",
    "                        # print(f\"        Finished {supp_filename_for_log}. Found {len(extracted_supp_primers)}. Total for PMCID {current_pmcid_str}: {primers_found_for_this_pmcid_count}\")\n",
    "            \n",
    "            except requests.HTTPError as e:\n",
    "                print(f\"    âš ï¸ HTTPError for {current_pmcid_str} (XML or Supplement download): {e}\")\n",
    "            except Exception as e_paper_processing: # This catches other errors during THIS paper's processing\n",
    "                print(f\"    âš ï¸ Unexpected error processing paper {current_pmcid_str}: {e_paper_processing}\")\n",
    "                import traceback\n",
    "                traceback.print_exc() # This will show where the NameError occurred if it's within this try\n",
    "                # Continue to the next paper\n",
    "            \n",
    "            time.sleep(0.3) # Politeness delay for Europe PMC\n",
    "        # --- End of loop for individual papers (epmc_hits_for_year) ---\n",
    "\n",
    "        # --- Save results for the CURRENT YEAR ---\n",
    "        if master_primer_list_for_year:\n",
    "            print(f\"\\n--- Consolidating and Saving Results for YEAR {year_to_process} ---\")\n",
    "            # ... (rest of your CSV saving logic for the year, which seems correct) ...\n",
    "            final_df_year = pd.DataFrame(master_primer_list_for_year)\n",
    "            if not final_df_year.empty:\n",
    "                expected_columns = ['PMCID', 'Gene', 'Sequence', 'Orientation', 'Source File', 'Page', 'Original Cell Text']\n",
    "                actual_columns_in_df = final_df_year.columns.tolist()\n",
    "                subset_for_duplicates = [col for col in expected_columns if col in actual_columns_in_df]\n",
    "                if not subset_for_duplicates: final_df_year.drop_duplicates(inplace=True, keep='first')\n",
    "                else: final_df_year.drop_duplicates(subset=subset_for_duplicates, inplace=True, keep='first')\n",
    "                print(f\"Total unique primer entries found for {year_to_process}: {len(final_df_year)}\")\n",
    "                # (Print and save logic)\n",
    "                try:\n",
    "                    output_dir = DATA_DIR / \"yearly_results\"; output_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    output_csv_path_year = output_dir / f\"master_extracted_primers_{year_to_process}.csv\"\n",
    "                    final_df_year.to_csv(output_csv_path_year, index=False)\n",
    "                    print(f\"  âœ… Saved {len(final_df_year)} unique extracted primers for {year_to_process} to {output_csv_path_year}\")\n",
    "                except Exception as e_csv: print(f\"  âŒ Error saving CSV for {year_to_process}: {e_csv}\")\n",
    "            else: print(f\"No primer data to save for {year_to_process} (DataFrame empty).\")\n",
    "        else: print(f\"No primers extracted for YEAR {year_to_process}.\")\n",
    "        print(f\"###### FINISHED PROCESSING FOR YEAR: {year_to_process} ######\")\n",
    "\n",
    "    print(\"\\n\\nBatch processing complete for all specified years.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a8e74c-17d0-4cc4-b075-70c693582fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for yearly CSV files in: data\\psc\\yearly_results\n",
      "Found 4 CSV files to process:\n",
      "  - master_extracted_primers_2022.csv\n",
      "  - master_extracted_primers_2023.csv\n",
      "  - master_extracted_primers_2024.csv\n",
      "  - master_extracted_primers_2025.csv\n",
      "\n",
      "Total rows loaded from all CSVs: 6522\n",
      "Processing 6522 rows for aggregation...\n",
      "\n",
      "Aggregation complete. Found 4751 unique primer sequences.\n",
      "\n",
      "Successfully saved aggregated primer data to: data\\psc\\yearly_results\\aggregated_primer_summary_with_details.csv\n",
      "\n",
      "--- Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter # Import Counter\n",
    "\n",
    "def aggregate_primer_data(input_dir: Path, output_file: Path):\n",
    "    \"\"\"\n",
    "    Reads all 'master_extracted_primers_YYYY.csv' files from the input directory,\n",
    "    aggregates primer sequences, counts occurrences, lists source PMCIDs,\n",
    "    Orientations, and Genes (with counts), and saves the result to a new CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_dir (Path): The directory containing the yearly CSV files.\n",
    "        output_file (Path): The path to save the aggregated CSV file.\n",
    "    \"\"\"\n",
    "    print(f\"Looking for yearly CSV files in: {input_dir}\")\n",
    "\n",
    "    csv_files = glob.glob(str(input_dir / \"master_extracted_primers_*.csv\"))\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {input_dir} matching the pattern 'master_extracted_primers_*.csv'.\")\n",
    "        print(\"Please ensure your yearly CSV files are in that directory and named correctly.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(csv_files)} CSV files to process:\")\n",
    "    for f in csv_files:\n",
    "        print(f\"  - {os.path.basename(f)}\")\n",
    "\n",
    "    all_dataframes = []\n",
    "    required_columns = ['Sequence', 'PMCID', 'Orientation', 'Gene']\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"Warning: Skipping file {os.path.basename(file_path)} as it's missing column(s): {', '.join(missing_cols)}.\")\n",
    "                continue\n",
    "            all_dataframes.append(df)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Warning: Skipping empty file {os.path.basename(file_path)}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "    if not all_dataframes:\n",
    "        print(\"No data loaded from CSV files. Aggregation cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    master_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    print(f\"\\nTotal rows loaded from all CSVs: {len(master_df)}\")\n",
    "\n",
    "    if master_df.empty:\n",
    "        print(\"The combined dataframe is empty. Nothing to aggregate.\")\n",
    "        return\n",
    "\n",
    "    # Standardize and clean data\n",
    "    master_df['Sequence'] = master_df['Sequence'].astype(str).str.upper()\n",
    "    master_df['PMCID'] = master_df['PMCID'].astype(str)\n",
    "    master_df['Orientation'] = master_df['Orientation'].astype(str).str.strip().fillna('Unknown')\n",
    "    master_df['Gene'] = master_df['Gene'].astype(str).str.strip().fillna('Unknown')\n",
    "    \n",
    "    master_df['Orientation'] = master_df['Orientation'].apply(lambda x: 'Unknown' if x == '' else x)\n",
    "    master_df['Gene'] = master_df['Gene'].apply(lambda x: 'Unknown' if x == '' else x)\n",
    "\n",
    "    master_df.dropna(subset=['Sequence', 'PMCID'], inplace=True)\n",
    "    master_df = master_df[master_df['Sequence'] != '']\n",
    "\n",
    "    print(f\"Processing {len(master_df)} rows for aggregation...\")\n",
    "\n",
    "    # --- MODIFIED AGGREGATION FOR GENES ---\n",
    "    def aggregate_gene_counts(series):\n",
    "        # Count occurrences of each gene in the series\n",
    "        counts = Counter(str(g) for g in series)\n",
    "        # Sort by count (descending), then by gene name (ascending) for tie-breaking\n",
    "        sorted_counts = sorted(counts.items(), key=lambda item: (-item[1], item[0]))\n",
    "        # Format as \"GeneName (count)\"\n",
    "        return ', '.join([f\"{gene} ({count})\" for gene, count in sorted_counts])\n",
    "\n",
    "    aggregated_data = master_df.groupby('Sequence').agg(\n",
    "        Occurrence_Count=('Sequence', 'size'),\n",
    "        Source_PMCIDs=('PMCID', lambda x: ', '.join(sorted(list(set(str(i) for i in x))))),\n",
    "        Source_Orientations=('Orientation', lambda x: ', '.join(sorted(list(set(str(i) for i in x))))),\n",
    "        Source_Genes=('Gene', aggregate_gene_counts) # Use the new aggregation function\n",
    "    ).reset_index()\n",
    "    # --- END MODIFICATION ---\n",
    "\n",
    "    aggregated_data = aggregated_data.sort_values(by='Occurrence_Count', ascending=False)\n",
    "\n",
    "    print(f\"\\nAggregation complete. Found {len(aggregated_data)} unique primer sequences.\")\n",
    "\n",
    "    try:\n",
    "        output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        aggregated_data.to_csv(output_file, index=False)\n",
    "        print(f\"\\nSuccessfully saved aggregated primer data to: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving aggregated data to {output_file}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT_DATA_DIR = Path(\"data/psc/yearly_results\") \n",
    "    AGGREGATED_OUTPUT_FILE = INPUT_DATA_DIR / \"aggregated_primer_summary_with_details.csv\"\n",
    "\n",
    "    aggregate_primer_data(INPUT_DATA_DIR, AGGREGATED_OUTPUT_FILE)\n",
    "\n",
    "    print(\"\\n--- Script Finished ---\")\n",
    "    # Example of how to quickly view the top results if needed:\n",
    "    # if AGGREGATED_OUTPUT_FILE.exists():\n",
    "    #     top_results_df = pd.read_csv(AGGREGATED_OUTPUT_FILE)\n",
    "    #     print(\"\\nTop 10 most frequent primers:\")\n",
    "    #     print(top_results_df.head(10).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03abf780-b777-4723-8ee1-c454f53c4363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
